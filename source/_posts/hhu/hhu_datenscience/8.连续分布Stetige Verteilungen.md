---
title: Daten_Science æ€»ç»“-8
date: 2022-08-12 22:44:45
categories : 
- hhu 
- Daten_Science
tags:
password: cocovv
---


# Stetige Verteilungen

:::{admonition} Definition
Sei $X \colon \Omega \to \mathbb{R}$ eine Zufallsvariable, d.h. $\Omega$ ein Wahrscheinlichkeitsraum (eine Menge $\Omega$ zusammen mit einer $\sigma$-Algebra und einem WahrscheinlichkeitsmaÃŸ $P_\Omega$) und $X$ eine Abbildung, sodass $P(X \in A) := P_\Omega(X^{-1}(A))$ fÃ¼r alle meÃŸbaren Mengen $A \subseteq \mathbb{R}$ definiert ist.

Wenn $\Omega$ kein diskreter Wahrscheinlichkeitsraum ist (z.B. weil $\Omega$ eine Ã¼berabzÃ¤hlbar unendliche Menge ist, etwa $\Omega = \mathbb{R}^n$), und $X$ unendlich viele mÃ¶gliche Werte annimmt, nennen wir $X$ eine *stetige Zufallsvariable*.
:::

```
å®šä¹‰
è®©ğ‘‹ âˆ¶ Î© â†’ â„ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå³ Î©æ˜¯ä¸€ä¸ªæ¦‚ç‡ç©ºé—´ï¼ˆä¸€ä¸ªé›†åˆÎ©è¿åŒä¸€ä¸ªğœä»£æ•°å’Œä¸€ä¸ªæ¦‚ç‡åº¦é‡ğ‘ƒÎ©ï¼‰ï¼Œğ‘‹æ˜¯ä¸€ä¸ªæ˜ å°„ï¼Œä½¿å¾—ğ‘ƒï¼ˆğ‘‹âˆˆğ´ï¼‰âˆ¶= ğ‘ƒÎ©ï¼ˆğ‘‹-1ï¼ˆğ´ï¼‰ï¼‰å¯¹äºæ‰€æœ‰å¯æµ‹é›†åˆğ´âŠ†â„æ˜¯å®šä¹‰ã€‚
å¦‚æœÎ©ä¸æ˜¯ä¸€ä¸ªç¦»æ•£çš„æ¦‚ç‡ç©ºé—´ï¼ˆä¾‹å¦‚å› ä¸ºÎ©æ˜¯ä¸€ä¸ªè¶…æ•°çš„æ— é™é›†ï¼Œæ¯”å¦‚Î©=â„ğ‘›ï¼‰ï¼Œå¹¶ä¸”ğ‘‹å…·æœ‰æ— é™å¤šçš„å¯èƒ½å€¼ï¼Œæˆ‘ä»¬ç§°ğ‘‹ä¸ºè¿ç»­éšæœºå˜é‡ã€‚

```

:::{admonition} Definition
Man nennt die kleinste Menge $A \subset \mathbb{R}$ mit $P(X \in A)=1$ den *TrÃ¤ger* von $X$ und schreibt auch

$$
supp(X) = \bigcap \{ A \subseteq \mathbb{R} : P(X \in A) = 1 \}
$$
:::

Der TrÃ¤ger ist auch fÃ¼r diskrete reelle Zufallsvariablen definiert - der offensichtliche Unterschied ist, dass per definitionem diskrete Zufallsvariablen einen diskreten TrÃ¤ger haben (d.h. eine Menge reeller Zahlen, die keinen HÃ¤ufungspunkt hat, insbesondere kein Intervall enthÃ¤lt).


```
è½½ä½“ä¹Ÿæ˜¯ä¸ºç¦»æ•£å®æ•°éšæœºå˜é‡å®šä¹‰çš„--æ˜æ˜¾çš„åŒºåˆ«æ˜¯ï¼Œæ ¹æ®å®šä¹‰ï¼Œç¦»æ•£éšæœºå˜é‡æœ‰ä¸€ä¸ªç¦»æ•£çš„è½½ä½“ï¼ˆå³ä¸€ä¸ªæ²¡æœ‰ç°‡ç‚¹çš„å®æ•°é›†ï¼Œç‰¹åˆ«æ˜¯ä¸åŒ…å«åŒºé—´ï¼‰ã€‚
8.1 å¯†åº¦å‡½æ•°

```



## Dichtefunktionen

:::{admonition} Definition
Sei $\Omega \subset \mathbb{R}^n$ und $f \colon \Omega \to [0,\infty)$ eine integrierbare Funktion (bezÃ¼glich des Lebesgue-MaÃŸes auf $\mathbb{R}^n$) mit $\int_\Omega f(x) dx = 1$. Dann ist auf $\Omega$ ein WahrscheinlichkeitsmaÃŸ definiert durch:

$$
\text{fÃ¼r } A \subseteq \Omega \text{ messbar ist } P(A) := \int_A f(x) dx
$$

Wir nennen fÃ¼r jedes WahrscheinlichkeitsmaÃŸ $P$, welches sich so schreiben lÃ¤sst, die Funktion $f$ eine *Wahrscheinlichkeitsdichtefunktion* (probability density function, pdf) oder kurz *Dichte* (gelegentlich Wahrscheinlichkeitsmassefunktion, pmf).
:::


```
å®šä¹‰
è®© Î© âŠ‚ â„ğ‘›å’Œ ğ‘“ âˆ¶ Î© â†’ [0, âˆ] æ˜¯ä¸€ä¸ªå¯æ•´å®šçš„å‡½æ•°ï¼ˆå…³äºâ„ğ‘›ä¸Šçš„ Lebesgue åº¦é‡ï¼‰ï¼Œâˆ«Î© ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ = 1ã€‚
å¯¹äºä»»ä½•æ¦‚ç‡åº¦é‡ğ‘ƒï¼Œå¯ä»¥è¿™æ ·å†™ï¼Œæˆ‘ä»¬ç§°å‡½æ•°ğ‘“ä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰æˆ–ç®€ç§°ä¸ºå¯†åº¦ï¼ˆå¶å°”ä¹Ÿæœ‰æ¦‚ç‡è´¨é‡å‡½æ•°ï¼Œpmfï¼‰ã€‚

```


Gegeben eine reelle Zufallsvariable $X \colon \Omega \to \mathbb{R}$, deren Verteilung $P_X$ durch eine Dichte $f$ gegeben ist, gilt also fÃ¼r $A \subseteq \mathbb{R}$

$$
\int_{X^{-1}(A)} d\omega = P(X \in A) = P_X(A) = \int_A f(x) dx
$$

Dieses rechte Integral ist nun ein Integral im Wertebereich von $X$.
Analog kÃ¶nnen wir auch den Erwartungswert und die Varianz berechnen:

$$
\mathbb{E}(X) = \int_\Omega X(\omega) d \omega = \int_{\mathbb{R}} x f(x) dx = \int_{-\infty}^{\infty} xf(x) dx
$$

$$
\mathbb{V}(X) &= \int_\Omega \left(X(\omega)-\mathbb{E}(X)\right)^2 d \omega \\
              &= \int_{-\infty}^{\infty} \left(x-\mathbb{E}(X)\right)^2 f(x) dx \\
              &= \int_{-\infty}^{\infty} x^2f(x) dx  - {\left( \mathbb{E}(X) \right)}^2
$$

:::{admonition} Beispiel
Seien $a < b \in \mathbb{R}$. Mit $\Omega = \mathbb{R}^1$ und $f \colon \mathbb{R} \to [0,\infty)$ gegeben durch

$$
f(x) = \begin{cases} \frac{1}{b-a}, & \text{ wenn } x \in [a,b] \\ 0 & \text{ sonst} \end{cases}
$$

ist ein WahrscheinlichkeitsmaÃŸ auf $\mathbb{R}$ definiert, die *stetige Gleichverteilung* auf dem Intervall $[a,b]$.
Der Erwartungswert (wir nutzen $\int x dx = \frac{x^2}{2}$) ist $\mathbb{E}(X) =$

$$
\int_{-\infty}^{\infty} xf(x) dx = \int_a^b \frac{xdx}{b-a} = \frac{1}{b-a} \left[\frac{x^2}{2}\right]_a^b = \frac{b^2 -a^2}{2(b-a)} = \frac{(b+a)(b-a)}{2(b-a)} = \frac{a+b}{2}
$$

Das zweite Moment ist

$$
\mathbb{E}(X^2) = \int_{-\infty}^\infty x^2 f(x)dx = \left[ \frac{x^3}{3(b-a)} \right]_a^b = \frac{b^3-a^3}{3(b-a)} = \frac{a^2+ab+b^2}{3}
$$

Die Varianz ist

$$
\mathbb{V}(X) = \mathbb{E}(X^2) - {\left(\mathbb{E}(X)\right)}^2 =  \frac{a^2+ab+b^2}{3} - \frac{(a+b)^2}{4} = \frac{(a-b)^2}{12}
$$
:::

:::{admonition} Beispiel
Nicht jedes WahrscheinlichkeitsmaÃŸ erlaubt eine Darstellung mit einer Dichte:
Betrachte auf $\mathbb{R}$ die Verteilung mit

$$
P(A) = \begin{cases} 1 & \text{ wenn } 0 \in A \\ 0 & \text{ sonst} \end{cases}
$$

Diese Verteilung heiÃŸt *Dirac-Verteilung* mit Masse bei $0$. Wenn man sich sehr viel MÃ¼he gibt, so etwas Ã¤hnliches wie eine Dichte zu basteln, so muss man zur Theorie der Distributionen aus der Funktionalanalysis greifen (Physiker kennen das). Es gibt auch Verteilungen, da genÃ¼gt auch keine Distribution.
:::

```
è¿™ç§åˆ†å¸ƒè¢«ç§°ä¸ºè´¨é‡ä¸º0çš„ç‹„æ‹‰å…‹åˆ†å¸ƒã€‚å¦‚æœä½ éå¸¸åŠªåŠ›åœ°è¯•å›¾åˆ›å»ºç±»ä¼¼äºå¯†åº¦çš„ä¸œè¥¿ï¼Œä½ å¿…é¡»æ±‚åŠ©äºå‡½æ•°åˆ†æä¸­çš„åˆ†å¸ƒç†è®ºï¼ˆç‰©ç†å­¦å®¶çŸ¥é“è¿™ä¸€ç‚¹ï¼‰ã€‚ä¹Ÿæœ‰ä¸€äº›åˆ†é…ï¼Œæ²¡æœ‰åˆ†é…æ˜¯è¶³å¤Ÿçš„ã€‚

```


:::{admonition} Definition
Sei $(\Omega, P)$ ein Wahrscheinlichkeitsraum, dann nennen wir fÃ¼r eine Zufallsvariable $X \colon \Omega \to \mathbb{R}$x die Funktion

ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒå‡½æ•°æˆ–cdfï¼ˆç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼‰ã€‚å®ƒå§‹ç»ˆå­˜åœ¨ï¼Œä¸å¯†åº¦ï¼ˆpdfï¼‰ç›¸åã€‚

$$
F \colon \mathbb{R} \to [0,1],\qquad x \mapsto P(X \leq x)
$$

die *kumulative Wahrscheinlichkeitsverteilungsfunktion* oder auch *Verteilungsfunktion* oder *cdf* (cumulative distribution function). Sie existiert immer, im Gegensatz zur Dichte (pdf).
:::

:::{admonition} Beispiel
Wenn $P_X$ eine Dichte $f \colon \mathbb{R} \to [0,\infty)$ hat,
also $P(a \leq X \leq b) = \int_a^b f(x)dx$, so ist

$$
F(x) = \int_{-\infty}^x f(x)dx.
$$
:::


```
ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒå‡½æ•°*æˆ–ä¹Ÿæ˜¯*åˆ†å¸ƒå‡½æ•°*æˆ–*cdf*ï¼ˆç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼‰ã€‚å®ƒå§‹ç»ˆå­˜åœ¨ï¼Œä¸å¯†åº¦ï¼ˆpdfï¼‰ç›¸åã€‚

```


:::{admonition} Definition
Sei $X$ eine reelle Zufallsvariable, dann heiÃŸt 

$$
M_X(t) := \mathbb{E}\left(e^{tX}\right)
$$

die *Momentenerzeugendenfunktion* von $X$
(wenn es ein kleines Intervall um $t=0$ gibt, sodass die entsprechenden Erwartungswerte fÃ¼r alle $t$ in diesem Intervall existieren).
:::

Wenn man die Taylor-Reihe der Exponentialfunktion $e^x$ anschaut, dort fÃ¼r $x$ einfach $tX$ einsetzt, und dann den Erwartungswert bildet, kann man den Erwartungswert der Summe als Summe der Erwartungswerte sehen (LinearitÃ¤t des Erwartungswerts) und so ist diese Reihe einfach nur eine andere Art, die Momente von $X$ zu verpacken. Das $n$-te Moment lÃ¤sst sich am Koeffizient von $t^n$ ablesen. Solche Erzeugendenfunktionen sind ein sehr nÃ¼tzliches Hilfsmittel in der Stochastik, aber auch in anderen kombinatorisch geprÃ¤gten Gebieten, insbesondere der Informatik.


```
å¦‚æœçœ‹ä¸€ä¸‹æŒ‡æ•°å‡½æ•°ğ‘’ğ‘¥çš„æ³°å‹’çº§æ•°ï¼Œç®€å•åœ°å°†é‚£é‡Œçš„ğ‘¡ğ‘‹æ›¿æ¢ä¸ºğ‘¥ï¼Œç„¶åå½¢æˆæœŸæœ›å€¼ï¼Œå¯ä»¥çœ‹åˆ°æœŸæœ›å€¼ä¹‹å’Œä¸ºæœŸæœ›å€¼ä¹‹å’Œï¼ˆæœŸæœ›å€¼çš„çº¿æ€§ï¼‰ï¼Œæ‰€ä»¥è¿™ä¸ªçº§æ•°åªæ˜¯å¯¹ğ‘‹çš„çŸ©çš„å¦ä¸€ç§åŒ…è£…æ–¹å¼ã€‚å¯ä»¥ä»ğ‘¡ğ‘›çš„ç³»æ•°ä¸­è¯»å‡ºç¬¬ğ‘›çš„æ—¶åˆ»ã€‚è¿™æ ·çš„ç”Ÿæˆå‡½æ•°åœ¨éšæœºå­¦ä¸­æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ï¼Œä½†åœ¨å…¶ä»–ç»„åˆé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è®¡ç®—æœºç§‘å­¦ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
```


:::{admonition} Definition
Sei $X$ eine reelle Zufallsvariable, dann heiÃŸt

$$
\phi_X(t) := \mathbb{E}\left( e^{itX} \right)
$$

die *charakteristische Funktion* von $X$.
:::

Klar: $\phi_X(-it) = M_X(t)$, allerdings sind die Konvergenzeigenschaften von $\phi_X$ besser, daher existiert diese Funktion als Funktion in einer reellen Variable $t$ immer.