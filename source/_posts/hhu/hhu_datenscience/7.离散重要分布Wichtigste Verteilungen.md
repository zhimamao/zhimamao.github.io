---
title: Daten_Science 总结-7
date: 2022-08-12 22:44:45
categories : 
- hhu 
- Daten_Science
tags:
password: cocovv
---

# Wichtigste Verteilungen


## Diskrete Verteilungen

Wir schreiben für einen endlichen diskreten Wahrscheinlichkeitsraum $\Omega = \{\omega_1,\dots,\omega_n\}$ nun $p(\omega) := P(\{\omega\})$.
Um die Begriffe Erwartungswert und Varianz auf Wahrscheinlichkeitsmaße anzuwenden, betrachte z.B. die Zufallsvariable $X \colon \Omega \to \mathbb{R}, \omega_i \mapsto i$ (das entspricht der Wahl einer Anordnung auf $\Omega$).

### Diskrete Gleichverteilung

Auf einer endlichen Menge $\Omega$ ist die *Gleichverteilung* gegeben durch
$p(\omega) = \frac{1}{n}$.
Sie ist charakterisiert als die Entropie-maximierende Verteilung auf $\Omega$, mit Entropie $H(X) = \log_2(n)$
Erwartungswert ist $\mathbb{E}(X) = \frac{n+1}{2}$ und Varianz $\mathbb{V}(X) = \frac{n^2-1}{12}$.

Unser Standardbeispiel ist ein fairer Würfel ($n=6$).

```
7.1 离散分布
现在我们把𝑝(𝜔)∶= 𝑃({𝜔})用于有限离散概率空间Ω={𝜔1, ..., 𝜔𝑛}。为了将期望值和方差的概念应用于概率度量，例如考虑随机变量𝑋 ∶ Ω → ℝ, 𝜔𝑖 ↦ 𝑖（这相当于在Ω上选择一个顺序）。
7.1.1 离散均匀分布
在一个有限集合Ω上，均匀分布由𝑝(𝜔)= . 它的特点是Ω上的熵最大分布，熵𝐻(𝑋)=log2(𝑛)期望值为𝔼(𝑋)=2𝑛+1，方差为

```

### Kategorielle Verteilung

分类分布

Die Parameter einer kategoriellen Verteilung auf $\Omega$ bestehen aus $n$ reellen Zahlen $p_i = p(\omega_i)$ unter den Bedingungen $0 \leq p_i \leq 1$ und $\sum_{i=1}^n p_i = 1$. Die letzte Bedingung schneidet eine Hyperebene aus dem $\mathbb{R}^n$ aus, also ist der Parameterraum $(n-1)$-dimensional. Die Ungleichungen umgrenzen in dieser Hyperebene einen Simplex.

Erwartungswert von $X$ ist $\mathbb{E}(X) = \sum_{i=1}^n i p_i$, was sich im Allgemeinen nicht weiter vereinfachen lässt. Offensichtlich ist die diskrete Gleichverteilung der Spezialfall $p_1 = \dots = p_n = \frac{1}{n}$ (der Mittelpunkt des Parameterraums). Ein anderer Spezialfall ist $n=2$, dann ist der Parameterraum $1$-dimensional.

Wenn die Kehrwerte $n_i := p_i^{-1}$ alle ganze Zahlen sind, lässt sich eine kategorielle Verteilung mit gefärbten Bällen in einer Urne, einmal ziehen ohne Zurücklegen, modellieren. Dabei ist $n_i$ die Anzahl der Bälle der Farbe $i$.

```
如果离散均匀分布是特例𝑝1 = ⋯ = 𝑝𝑛 = （参数空间的中心）。另一种特殊情况是𝑛=2，那么参数空间是一维的。
如果倒数𝑛𝑖∶= 𝑝𝑖-1都是整数，那么可以用瓮中的彩球来模拟分类分布，抽一次不留后路。这里𝑛𝑖是颜色为𝑖的球的数量。
```
### Bernoulli-Verteilung

Für $n=2$ ist die Bernoulli-Verteilung zu $p$ ein anderer Name für die kategorielle Verteilung mit $p_1 = p$ und $p_2 = 1-p$.

Sie modelliert den Münzwurf mit einer potentiell unfairen Münze.
Der Erwartungswert für $Y \colon \{\omega_1,\omega_2\} \to \mathbb{R}$ mit $ Y(\omega_1)=1$ und $Y(\omega_2)=0$ ist $\mathbb{E}(Y) = p$ und die Varianz ist $\mathbb{V}(Y) = p - p^2 = p(1-p)$.

Die Entropie ist

$$
H(Y) = -p\log_2(p) - (1-p)\log_2(1-p).
$$

Wenn man diesen Ausdruck nach $p$ ableitet und die Ableitung 

$$
\dfrac{\partial H(Y)}{\partial p} = -\log_2\left( \dfrac{p}{1-p} \right)
$$
auf Nullstellen untersucht, findet man diese bei $p=\frac{1}{2}$, denn nur für $x=1$ ist $\log_2(x)=0$. Das beweist, dass ein fairer Münzwurf die maximale Entropie unter allen Münzwürfen hat.

```
.1.3 伯努利分布
对于𝑛=2，对𝑝的伯努利分布是分类分布的另一个名称，其中𝑝1=𝑝，𝑝2=1 - 𝑝。
它用一个可能不公平的硬币来模拟掷硬币。𝑌的预期值∶{𝜔1, 𝜔2}。→ ℝ（𝜔1）=1，𝑌（𝜔2）=0，则𝔼（𝑌）=𝑝，方差为𝕍（𝑌）=𝑝 - 𝑝2=𝑝（1 - 𝑝）。
```

### Binomialverteilung

Wenn man einen Münzwurf (ein Bernoulli-Experiment) mit Parameter $p$ $n$-mal wiederholt, und die Summe über die Ergebnisse bildet, erhält man eine Zufallsvariable $X \colon \Omega \to \mathbb{R}$ (die genaue Form von $\Omega$ ist nicht wichtig, es funktioniert aber z.B. $\{0,1\}^n$ mit $P(x)=p^k(1-p)^{n-k}$ wenn $x$ genau $k$ Einträge gleich $1$ hat).

Es ist $P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}$, denn es gibt genau $\binom{n}{k}$ Möglichkeiten, $k$ Einsen in einen Vektor der Dimension $n$ zu schreiben.

Der Erwartungswert ist $\mathbb{E}(X) = np$ und die Varianz ist $\mathbb{V}(X) = np(1-p)$. Vorsicht: die Entropie ist relativ aufwändig präzise auszurechnen.

Wenn die Kehrwerte $p^{-1},\ (1-p)^{-1}$ ganze Zahlen sind, kann man die Binomialverteilung im Urnenmodell mit zwei verschiedenfarbigen Bällen modellieren, die im entsprechenden Verhältnis $p : (1-p)$ in der Urne liegen. $n$-fach ziehen mit zurücklegen (ungeordnete Stichprobe) ist dann Binomialverteilt.

$$
X \sim Bin(n,p)
$$

```
.1.4 二项式分布
如果你用参数𝑝重复掷硬币（伯努利实验），并对结果进行求和，你会得到一个随机变量 ∶ Ω → ℝ（Ω的确切形式并不重要，但它可以作为例子。 例如，{0，1}𝑛与𝑃（𝑥）=𝑝𝑘（1-𝑝）𝑛-𝑘如果𝑥正好有𝑘项等于1）。
它是𝑃（𝑋=𝑘）=（ ）𝑝（1-𝑝）𝑛-𝑘，因为正好有（ ）种方法可以把𝑘的写成维数𝑛的向量。
预期值为𝔼（𝑋）=𝑛𝑝，方差为𝕍（𝑋）=𝑛𝑝（1-𝑝）。注意：熵是相对复杂的，难以精确计算。
如果倒数𝑝-1，(1-𝑝)-1是整数，我们可以用两个不同颜色的球以相应的比例𝑝∶(1-𝑝)躺在瓮中的二项分布模型。𝑛-折叠抽样与放回（无序样本）是二项分布。
```
### Hypergeometrische Verteilung

Die Lotto-Verteilung: im Urnenmodell $n$-fach ziehen von $2$ verschiedenen Ballfarben, davon $K$ der einen Farbe und $N-K$ der anderen Farbe, *ohne* Zurücklegen. Dann ist

$$
P(X = k) = \dfrac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}.
$$

Der Erwartungswert ist $\mathbb{E}(X) = n\frac{K}{N}$ (vergleiche die Binomialverteilung) und die Varianz $\mathbb{V}(X) = \dfrac{K(N-K)(N-n)}{N^2(N-1)}$.

$$
X \sim Hyp(N,K,n)
$$

```
.1.5 超几何分布
乐透分布：在瓮模型中，从2个不同颜色的球中抽取𝑛倍数，其中𝐾为一种颜色，𝑁 - 𝐾为另一种颜色，不需要铺设。那么
```

### Poisson-Binomialverteilung

Dies ist die Verallgemeinerung des $n$-fachen Münzwurfs dahingehend, dass für jeden Wurf eine andere Münze verwendet wird (also sind die Parameter $p_1,\dots,p_n$ und der Spezialfall $p_1 = \dots = p_n$ ist die Binomialverteilung).

Der Erwartungswert ist $\mathbb{E}(X) = \sum_{i=1}^n p_i$ und die Varianz $\mathbb{V}(X) = \sum_{i=1}^n (1-p_i)p_i$.

$$
P(X=k) = \sum_{i=1} \binom{n}{k} p_i^k (1-p_i)^{n-k}
$$

Die Entropie einer Poisson-Binomialverteilung ist kleinergleich einer entsprechenden Binomialverteilung mit gleichem Erwartungswert (je gleicher die $p_i$, desto höher die Entropie).

Wenn wir es mit einer Parameterfolge $(p_i)_{i\geq 1}$ zu tun haben (also ein unendlich oft wiederholter Münzwurf mit lauter verschiedenen Münzen), sodass die Folge $(ip_i)_{i \geq 1}$ gegen eine endliche Zahl $\lambda > 0$ konvergiert (dazu muss $p_i$ für $i \to \infty$ ungefähr so schrumpfen wie $\frac{1}{i}$), so lässt sich eine gute Approximation finden:

$$
\lim_{n\to \infty} \binom{n}{k} p_n^k(1-p_n)^{n-k} = e^{-\lambda} \dfrac{\lambda^k}{k!}
$$

Diese Approximation gilt also dann, wenn der "positive Ausgang" eines beliebig oft wiederholten Experiments (schnell genug) immer unwahrscheinlicher wird.

```
.1.6 泊松二项分布
这是对𝑛掷硬币的概括，即每次掷硬币都使用不同的硬币（所以参数为𝑝1, ..., 𝑝𝑛，特殊情况下𝑝1 = ⋯ = 𝑝𝑛是二项分布）。

泊松二项分布的熵小于或等于具有相同期望值的相应二项分布（𝑝𝑖越相等，熵越大）。
如果我们处理的是一个参数序列(𝑝𝑖)𝑖≥1(即用响亮的不同硬币无限次重复抛掷硬币)，使序列(𝑖𝑝𝑖)𝑖≥1收敛到一个有限数𝜆>0(为此，𝑝𝑖必须对𝑖→∞近似缩小)，可以找到一个好的近似值

因此，当一个实验重复任何次数（足够快）的 "积极结果 "变得越来越不可能时，这个近似值就适用了。
```

### Poisson-Verteilung

Mit einem Parameter $\lambda > 0$ ist $X \colon \Omega \to \mathbb{R}$ *poissonverteilt*, wenn

$$
P(X=k) = \dfrac{e^{-\lambda}\lambda^k}{k!}
$$

$\mathbb{E}(X) = \lambda = \mathbb{V}(X)$.

Diese Verteilung modelliert, wie viele Ereignisse in einem festen Zeitraum eintreten, wobei $\lambda$ die konstante mittlere Rate ist, und diese Ereignisse voneinander unabhängig sind.

Wenn Sie überrascht werden wollen, schauen Sie sich das *Wartezeitparadox* an, was erklärt, warum Sie länger auf den Bus warten, als Sie bisher vielleicht angenommen haben. Dabei geht es auch um eine Poissonverteilung.

```
.1.7 泊松分布
在参数𝜆>0的情况下，𝑋 ∶ Ω → ℝ是泊松分布，如果
这个分布模拟了在一个固定的时间段内有多少事件发生，其中𝜆是恒定的平均速率，这些事件是相互独立的。
如果你想感到惊讶，可以看看等待时间悖论，它解释了为什么你等待公交车的时间比你之前想象的要长。这也涉及到泊松分布。
```